{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_drone\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from statistics import mean\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0a92ca5b5455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CartPole-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"notebook\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.95,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.00025,\n",
    "            \"momentum\": 0.95,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24\n",
    "}\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "env.render(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, env, parameters):\n",
    "        self.env = env\n",
    "        self.buffer_size = parameters[\"buffer_size\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        # A deque is a buffer which can append and pop from\n",
    "        # both sides. If it is at max length, any appends\n",
    "        # will discard the first value to maintain size.\n",
    "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
    "        # Every transition is stores as a named tuple.\n",
    "        self.experience_tuple = namedtuple(\"Experience\", \n",
    "                                           field_names=[\"state\", \n",
    "                                                        \"action\", \n",
    "                                                        \"reward\", \n",
    "                                                        \"done\", \n",
    "                                                        \"next_state\"])\n",
    "        \n",
    "    def push(self, state, action, reward, done, next_state):\n",
    "        transition = self.experience_tuple(state=state, \n",
    "                                           action=action, \n",
    "                                           reward=reward, \n",
    "                                           done=done, \n",
    "                                           next_state=next_state)\n",
    "        self.experience_buffer.append(transition)\n",
    "        \n",
    "    def sample(self):\n",
    "        sample_size = min(len(self.experience_buffer), self.batch_size)\n",
    "        experiences = random.sample(self.experience_buffer, sample_size)\n",
    "        \n",
    "        states = np.asarray([experience.state for experience in experiences])\n",
    "        actions = np.asarray([experience.action for experience in experiences])\n",
    "        rewards = np.asarray([experience.reward for experience in experiences])\n",
    "        dones = np.asarray([experience.done for experience in experiences])\n",
    "        next_states = np.asarray([experience.next_state for experience in experiences])\n",
    "        \n",
    "        batch = self.experience_tuple(\n",
    "            state=torch.from_numpy(states).type(Tensor),\n",
    "            action=torch.from_numpy(actions).type(LongTensor),\n",
    "            reward=torch.from_numpy(rewards).type(Tensor),\n",
    "            done=torch.from_numpy(dones).type(ByteTensor),\n",
    "            next_state=torch.from_numpy(next_states).type(Tensor)\n",
    "        )\n",
    "        \n",
    "        return batch\n",
    "        #return (states, actions, rewards, dones, new_states)\n",
    "        \n",
    "    def warm_up(self):\n",
    "        for _ in range(10):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.env.action_space.sample()\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.push(state, action, reward, done, next_state)\n",
    "                state = next_state\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.experience_buffer)\n",
    "\n",
    "#test = ExperienceReplay(env, parameters)\n",
    "#test.warm_up()\n",
    "#test.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
    "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        # Define network layers\n",
    "        self.fc1 = nn.Linear(self.observations_size, self.hidden_layer_1)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_1, self.hidden_layer_2)\n",
    "        self.fc3 = nn.Linear(self.hidden_layer_2, self.action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward propagation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_loss(self):\n",
    "        return nn.MSELoss()\n",
    "        \n",
    "    def get_optim(self, lr):\n",
    "        return optim.Adam(self.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_input_format(env, coordinates):\n",
    "    # observation_space is a tuple of discrete numbers\n",
    "    # with a certain amount of possible numbers.\n",
    "    observation_size = sum([x.n for x in env.observation_space])\n",
    "    output = [0] * observation_size\n",
    "    \n",
    "    start_at = 0\n",
    "    for index, coordinate in enumerate(coordinates):\n",
    "        output[coordinate + start_at] = 1\n",
    "        start_at += env.observation_space[index].n\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) = r(s,a) + \\gamma \\max\\limits_{a}Q(s',a)$ \\\n",
    "$θ_{target}=τ*θ_{local}+(1-τ)*θ_{target}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.env = env\n",
    "        self.local_network = DQN(env, parameters)\n",
    "        self.target_network = deepcopy(self.local_network)\n",
    "        #self.target_network = DQN(env, parameters)\n",
    "        self.experience_replay = ExperienceReplay(env, parameters)\n",
    "        \n",
    "        self.epsilon = parameters[\"epsilon_init\"]\n",
    "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
    "        self.epsilon_minimum = parameters[\"epsilon_minimum\"]\n",
    "        self.tau = parameters[\"tau\"]\n",
    "        self.gamma = parameters[\"gamma\"]\n",
    "        self.epochs = parameters[\"epochs\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
    "        \n",
    "    def update_local_network(self):\n",
    "        #states, actions, rewards, dones, next_states = self.experience_replay.sample()\n",
    "        batch = self.experience_replay.sample()\n",
    "        \n",
    "        state_batch = Variable(batch.state)\n",
    "        action_batch = Variable(batch.action)\n",
    "        reward_batch = Variable(batch.reward)\n",
    "        \n",
    "        \n",
    "        non_final = LongTensor([i for i, done in enumerate(batch.done) if not done])\n",
    "        non_final_mask = (1 - batch.done).bool()\n",
    "        # To prevent backprop through the target action values, set volatile=False (also sets requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            non_final_next_states = Variable(batch.next_state.index_select(0, non_final))\n",
    "\n",
    "            # Compute Q(s_t, a), the estimated Q-values, using local network\n",
    "            Q_state_action = self.local_network(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            V_next_state = Variable(torch.zeros(self.experience_replay.batch_size).type(Tensor))\n",
    "            _, next_state_actions = self.local_network(non_final_next_states).max(1, keepdim=True)\n",
    "            V_next_state[non_final_mask] = self.target_network(non_final_next_states).gather(1, next_state_actions)\n",
    "        \n",
    "        return\n",
    "        \n",
    "        # Compute the target Q values\n",
    "        target_Q_state_action = reward_batch + (self.gamma * V_next_state)\n",
    "        \n",
    "        # Update Q_values with \"correct\" Q-values calculated using the Q-learning algorithm    \n",
    "        Q_local_expected = Q_local.clone()\n",
    "        for row, col_id in enumerate(actions):\n",
    "            Q_local_expected[row, col_id[0]] = Q_calc[row]\n",
    "        \n",
    "        # Train network by minimizing the difference between Q_local and modified Q_local \n",
    "        loss = self.local_network.get_loss()\n",
    "        optimizer = self.local_network.get_optim(self.learning_rate)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_size = loss(Q_local, Q_local_expected)\n",
    "        #print(loss_size)\n",
    "        loss_size.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        local_weights = self.local_network.state_dict()\n",
    "        target_weights = self.target_network.state_dict()\n",
    "        \n",
    "        for layer in local_weights:\n",
    "            target_weights[layer] = self.tau * local_weights[layer] + (1 - self.tau) * target_weights[layer]\n",
    "        \n",
    "        self.target_network.load_state_dict(target_weights)\n",
    "        \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon >= self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def select_action(self, state):\n",
    "        if self.epsilon > np.random.uniform():\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            #network_input = discrete_to_input_format(self.env, state)\n",
    "            network_input = state\n",
    "            input_tensor = torch.Tensor(network_input).squeeze(0)\n",
    "            action = self.local_network(input_tensor).max(0)[1].item()\n",
    "            #print(self.local_network(input_tensor))\n",
    "        return action\n",
    "\n",
    "    def step(self, state):\n",
    "        action = self.select_action(state)\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        return action, reward, done, next_state\n",
    "\n",
    "test = Agent(env, parameters)\n",
    "test.experience_replay.warm_up()\n",
    "#test.epsilon = 0\n",
    "#test.select_action(test.env.reset())\n",
    "print(\"lol\")\n",
    "test.update_local_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, iterations, episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_reward_list, iterations_list = [], []\n",
    "    agent.experience_replay.warm_up()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward=0\n",
    "        \n",
    "        if (episode != 0): \n",
    "            agent.update_epsilon()\n",
    "    \n",
    "        #agent.target_network.load_state_dict(agent.local_network.state_dict())\n",
    "        \n",
    "        for iteration in range(iterations):\n",
    "            action, reward, done, new_state = agent.step(state)\n",
    "            agent.experience_replay.push(state, action, reward, done, new_state)\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            agent.update_local_network()\n",
    "            agent.update_target_network()\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        iterations_list.append(iteration+1)\n",
    "        \n",
    "        if episode % 5 == 0 and episode != 0:\n",
    "            print(\"Episode: {0:d}-{1:d} | Avg. iterations: {2:0.2f}  | Max total reward: {3:0.2f} | Avg. total reward: {4:0.2f} | Epsilon: {5:0.4f}\" \\\n",
    "                  .format(episode-10, episode, mean(iterations_list), max(total_reward_list), mean(total_reward_list), agent.epsilon))\n",
    "            total_reward_list.clear()\n",
    "            iterations_list.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.95,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24\n",
    "}\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "dqn_agent = Agent(env, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(dqn_agent, 500, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
