{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, onnx, torch, torchvision\n",
    "import os, glob, torch, csv\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean(abs(output - target)) * 10\n",
    "    return loss\n",
    "\n",
    "print(my_loss(torch.tensor(1), torch.tensor(0.9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepStart network definition\n",
    "\n",
    "DeepStar consists of two layer of max pooled convolutional neural networks that is then fed into two layers of normal neural networks.\n",
    "\n",
    "**Get Loss**: Currently we use a custom loss function that is just the difference between the expected and output. Times 10 to keep the learning rate acceptable.<br>\n",
    "**Get Optim**: Currentl we are using the Adadelta loss function.http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepStar(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepStar, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=16, stride=2, padding=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=8, stride=2, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=1, padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        #self.conv_dropout = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(256 * 12 * 12 + 4, 64)\n",
    "        self.fc2 = torch.nn.Linear(64 + 4, 2)\n",
    "        \n",
    "    def __name__(self):\n",
    "        return \"DeepStar\"\n",
    "    \n",
    "    def get_loss(self):\n",
    "        return lambda o ,t: torch.mean(abs(o - t)) * 10\n",
    "        #return torch.nn.L2Loss()\n",
    "        \n",
    "    def get_optim(self, rho, lr, weight_decay):\n",
    "        return optim.Adadelta(self.parameters(), rho=rho, lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pos_tensor = x[0:4]\n",
    "        x = x[4]\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        #x = self.conv_dropout(x)\n",
    "        x = x.view(-1, 256 * 12 * 12)\n",
    "        \n",
    "        new_pos_tensor = []\n",
    "        for i in range(len(x)):\n",
    "            new_pos_tensor.append([\n",
    "                pos_tensor[0][i].item(),\n",
    "                pos_tensor[1][i].item(),\n",
    "                pos_tensor[2][i].item(),\n",
    "                pos_tensor[3][i].item()])\n",
    "        \n",
    "        new_pos_tensor = torch.tensor(new_pos_tensor)\n",
    "        \n",
    "        x = torch.cat((x, new_pos_tensor), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(torch.cat((x, new_pos_tensor), 1))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "Because we use a custom dataset we have to have a custom data loader. The image has a heightmap in the first axis and the second and third axis represents the start and end point. The image name also contains the correct midpoint for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathDataLoader(utils.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.map = f'{data_dir}/map.png'\n",
    "        self.data_path = f'{data_dir}/data.csv'\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.data = pd.read_csv(self.data_path, encoding = \"UTF-8\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data[\"Start\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with Image.open(self.map) as img:\n",
    "            sx, sy = self.to_tuple(self.data[\"Start\"][idx])\n",
    "            ex, ey = self.to_tuple(self.data[\"Stop\"][idx])\n",
    "            mx, my = self.to_tuple(self.data[\"Midpoint\"][idx])\n",
    "\n",
    "            img_tensor = self.to_tensor(img)\n",
    "\n",
    "            return ([sx / 256, sy / 256, ex / 256, ey / 256, img_tensor], (mx / 256, my / 256))\n",
    "    \n",
    "    def to_tuple(self, t):\n",
    "        return tuple(map(int, t.replace('(','').replace(')', '').split(', '))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
